{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GaussLayer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btekgit/GaussianLayer/blob/master/GaussLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLV2ztzfHsl3",
        "colab_type": "code",
        "outputId": "ce26cfde-5b06-4ffb-8571-310c625509c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# CODE for Gaussian Layer\n",
        "!pwd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXCGJ9NPH1NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Feb 13 19:07:34 2018\n",
        "LAst update Jun 17 2019\n",
        "\n",
        "@author: btek\n",
        "\"\"\"\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.utils import conv_utils\n",
        "from keras import activations, regularizers, constraints\n",
        "from keras import initializers\n",
        "from keras.engine import InputSpec\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU5r1jH0H5af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def idx_init(shape, dtype='float32'):\n",
        "    idxs = np.zeros((shape[0], shape[1]),dtype)\n",
        "    c = 0\n",
        "    # assumes square filters\n",
        "    \n",
        "    wid = np.int(np.sqrt(shape[0]))\n",
        "    hei =np.int(np.sqrt(shape[0]))\n",
        "    f = np.float32\n",
        "    for x in np.arange(wid):  # / (self.incoming_width * 1.0):\n",
        "        for y in np.arange(hei):  # / (self.incoming_height * 1.0):\n",
        "            idxs[c, :] = np.array([x/f(wid-1), y/f(hei-1)],dtype)\n",
        "            c += 1\n",
        "\n",
        "    return idxs\n",
        "\n",
        "def cov_init(shape, dtype='float32'):\n",
        "    \n",
        "    cov = np.identity(shape[1], dtype)\n",
        "    # shape [0] must have self.incoming_channels * self.num_filters\n",
        "    cov = np.repeat(cov[np.newaxis], shape[0], axis=0)\n",
        "    \n",
        "    #for t in range(shape[0]):\n",
        "    #    cov[t] = cov[t]\n",
        "    return cov\n",
        "\n",
        "def scale_init(shape, dtype='float32'):\n",
        "    #sc = np.linspace(0.5, 1.6, shape[0]) #best for mnist cluttered\n",
        "    #sc = np.linspace(0.05, 0.1, shape[0],dtype=dtype) #best for mnist cluttered\n",
        "    #sc = 0.05*np.ones(shape[0],dtype=dtype) #best for mnist cluttered\n",
        "    sc = np.linspace(0.05, 0.1, shape[0],dtype=dtype)#tried on fashion mnist with no difference\n",
        "    #sc=np.expand_dims(sc, axis=1)\n",
        "    #sc=np.expand_dims(sc, axis=2)\n",
        "    #print(sc)\n",
        "    return sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsUuLKvKH8V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussScaler(Layer):\n",
        "    def __init__(self, rank, filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='valid',\n",
        "                 data_format=None,\n",
        "                 dilation_rate=1,\n",
        "                 activation=None,\n",
        "                 use_bias=False,\n",
        "                 kernel_regularizer=None,\n",
        "                 gain=1.0,\n",
        "                 output_padding=None,\n",
        "                 **kwargs):\n",
        "        super(GaussScaler, self).__init__(**kwargs)\n",
        "        #def __init__(self, num_filters, kernel_size, incoming_channels=1, **kwargs):\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = data_format\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
        "        self.gain = gain\n",
        "                 \n",
        "        #self.input_shape = input_shape\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'))\n",
        "        print(kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.num_filters = filters\n",
        "        #self.incoming_channels = incoming_channels\n",
        "        \n",
        "        \n",
        "        self.output_padding = output_padding\n",
        "        if self.output_padding is not None:\n",
        "            self.output_padding = conv_utils.normalize_tuple(\n",
        "                self.output_padding, 2, 'output_padding')\n",
        "            for stride, out_pad in zip(self.strides, self.output_padding):\n",
        "                if out_pad >= stride:\n",
        "                    raise ValueError('Stride ' + str(self.strides) + ' must be '\n",
        "                                     'greater than output padding ' +str(self.output_padding))\n",
        "                    \n",
        "        super(GaussScaler, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        \n",
        "        self.input_channels = input_dim\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "        print(\"kernel shape:\",kernel_shape)\n",
        "\n",
        "        self.bias = None\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        \n",
        "        kernel_size = self.kernel_size\n",
        "        # Idxs Init\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #mu = np.array([kernel_size[0] // 2, kernel_size[1] // 2])\n",
        "        mu = np.array([0.5, 0.5])\n",
        "\n",
        "\n",
        "        # Convert Types\n",
        "        self.mu = mu.astype(dtype='float32')\n",
        "\n",
        "        # Shared Parameters\n",
        "        # below works for only two dimensional cov \n",
        "        #self.cov = self.add_weight(shape=[input_dim*self.filters,2,2], \n",
        "        #                          name=\"cov\", initializer=cov_init, trainable=False)\n",
        "        \n",
        "        \n",
        "        self.cov_scaler = self.add_weight(shape=(self.filters,),\n",
        "                                          name='scaler',initializer=scale_init,\n",
        "                                          trainable=True,\n",
        "                                          constraint= constraints.NonNeg())\n",
        "                                  #constraint=constraints.non_neg())\n",
        "        \n",
        "        #print(\"Self.cov:\",self.cov)\n",
        "        #print(\"Self cov-scaler\",self.cov_scaler)\n",
        "        \n",
        "        # below prepares a meshgrid. \n",
        "        #self.idxs = self.add_weight(shape=[kernel_size[0]*kernel_size[1],2], \n",
        "        #                           name=\"idxs\", initializer=idx_init, trainable=False)\n",
        "        \n",
        "        self.idxs= idx_init(shape=[kernel_size[0]*kernel_size[1],2])\n",
        "        \n",
        "        super(GaussScaler, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "        \n",
        "    \n",
        "    def U(self):\n",
        "  \n",
        "        e1 = (self.idxs - self.mu)\n",
        "        #print(\"e1.shape\",e1.shape)\n",
        "        #print(\"cov scaler shape\",self.cov_scaler)\n",
        "   \n",
        "        #print(self.cov.shape)\n",
        "        #print(len(tf.unstack(self.cov,axis=0)))\n",
        "        #print( tf.linalg.inv(tf.unstack(self.cov,axis=0)[0]))\n",
        "        # tensorflow does not need scan it does the same op to all covs.\n",
        "        #cov_inv = self.cov\n",
        "        #cov_scaled =self.cov_scaler*self.cov\n",
        "#        cov_scaled = tf.scalar_mul(self.cov_scaler,self.cov)\n",
        "#        print(self.cov.shape, self.cov_scaler.shape )\n",
        "#        cov_scaled = K.batch_dot(self.cov_scaler,self.cov, axes=[1,2])\n",
        "        #cov_inv = tf.linalg.inv(cov_scaled)\n",
        "        #print(\"cov_scaled :\",cov_scaled.shape)\n",
        "        #cov_inv = K.map_fn(lambda x: tf.linalg.inv(x), elems=tf.unstack(self.cov,axis=0))\n",
        "       \n",
        "\n",
        "        #e2 = K.dot(e1, K.transpose(cov_inv))\n",
        "        #ex = K.batch_dot(e2, e1, axes=[[1], [1]])\n",
        "        #result = K.exp(-(1 / 2.0) * ex)\n",
        "\n",
        "        up= K.sum((self.idxs - self.mu)**2, axis=1)\n",
        "        #print(\"up.shape\",up.shape)\n",
        "        up = K.expand_dims(up,axis=1,)\n",
        "        #print(\"up.shape\",up.shape)\n",
        "        # clipping scaler in range to prevent div by 0 or negative cov. \n",
        "        cov_scaler = K.clip(self.cov_scaler,0.01,5)\n",
        "        #cov_scaler = self.cov_scaler\n",
        "        dwn = 2 * (cov_scaler ** 2)\n",
        "        #scaler = (np.pi*self.cov_scaler**2) * (self.idxs.shape[0])\n",
        "        result = K.exp(-up / dwn)\n",
        "        \n",
        "\n",
        "\n",
        "        # Transpose is super important.\n",
        "        #filter: A 4-D `Tensor` with the same type as `value` and shape\n",
        "        #`[height, width, output_channels, in_channels]`\n",
        "        # we do not care about input channels\n",
        "        \n",
        "        masks = K.reshape(result,(self.kernel_size[0],\n",
        "                                  self.kernel_size[1],\n",
        "                                  self.filters,1))   \n",
        "            \n",
        "        #sum normalization each filter has sum 1\n",
        "        #sums = K.sum(masks**2, axis=(0, 1), keepdims=True)\n",
        "        #print(sums)\n",
        "        #gain = K.constant(self.gain, dtype='float32')\n",
        "        masks /= K.sqrt(K.sum(K.square(masks), axis=(0, 1),keepdims=True))\n",
        "        #masks /= K.sum(masks, axis=(0, 1),keepdims=True)\n",
        "        #masks /= (self.kernel_size[0]*self.kernel_size[1])\n",
        "        \n",
        "        #masks *= (gain*np.sqrt(self.kernel_size[0]*self.kernel_size[1]))\n",
        "        #ums = sums * sums\n",
        "        #print(\"sums shape: \", sums.shape)\n",
        "        \n",
        "        # Sum normalisation\n",
        "        \n",
        "        #masks = masks * (gain/K.sqrt(sums))\n",
        "        #masks = masks * (gain/sums)\n",
        "        #print(\"masks shape\", masks.shape)\n",
        "        #print(\"masks mask\", K.mean(masks))\n",
        "        return masks\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "          h_axis, w_axis = 2, 3\n",
        "          c_axis= 1\n",
        "          \n",
        "        else:\n",
        "            h_axis, w_axis = 1, 2\n",
        "            c_axis=3\n",
        "            \n",
        "        ##BTEK \n",
        "        kernel = self.U()\n",
        "        in_channels =input_shape[c_axis]\n",
        "        \n",
        "        height, width = input_shape[h_axis], input_shape[w_axis]\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "        if self.output_padding is None:\n",
        "            out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "            out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        # Infer the dynamic output shape:\n",
        "        out_height = conv_utils.deconv_length(height,\n",
        "                                              stride_h, kernel_h,\n",
        "                                              self.padding,\n",
        "                                              out_pad_h,\n",
        "                                              self.dilation_rate[0])\n",
        "        out_width = conv_utils.deconv_length(width,\n",
        "                                             stride_w, kernel_w,\n",
        "                                             self.padding,\n",
        "                                             out_pad_w,\n",
        "                                             self.dilation_rate[1])\n",
        "        if self.data_format == 'channels_first':\n",
        "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
        "        else:\n",
        "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
        "\n",
        "        ##BTEK \n",
        "        kernel = self.U()\n",
        "        print(\"kernel shape in output:\",kernel.shape)\n",
        "        print(\"channel axis\")\n",
        "        kernel = K.repeat_elements(kernel, self.input_channels, axis=c_axis)\n",
        "        print(\"kernel reshaped :\",kernel.shape)\n",
        "        #---------------------------------------------------------------------\n",
        "        outputs = K.conv2d_transpose(\n",
        "            inputs,\n",
        "            kernel,\n",
        "            output_shape,\n",
        "            self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        if self.data_format == 'channels_first':\n",
        "            c_axis, h_axis, w_axis = 1, 2, 3\n",
        "        else:\n",
        "            c_axis, h_axis, w_axis = 3, 1, 2\n",
        "\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "        stride_h, stride_w = self.strides\n",
        "        if self.output_padding is None:\n",
        "            out_pad_h = out_pad_w = None\n",
        "        else:\n",
        "            out_pad_h, out_pad_w = self.output_padding\n",
        "\n",
        "        output_shape[c_axis] = self.filters\n",
        "        output_shape[h_axis] = conv_utils.deconv_length(output_shape[h_axis],\n",
        "                                                        stride_h,\n",
        "                                                        kernel_h,\n",
        "                                                        self.padding,\n",
        "                                                        out_pad_h,\n",
        "                                                        self.dilation_rate[0])\n",
        "        output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n",
        "                                                        stride_w,\n",
        "                                                        kernel_w,\n",
        "                                                        self.padding,\n",
        "                                                        out_pad_w,\n",
        "                                                        self.dilation_rate[1])\n",
        "        return tuple(output_shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfRXsUkkIDKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4K8pEZzIXOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # LOAD DATA\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDSdnMlILOk8",
        "colab_type": "code",
        "outputId": "553ff3b5-8b96-4268-94a7-e143bc51046d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "num_classes = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4BQMgF7LVHV",
        "colab_type": "text"
      },
      "source": [
        "CREATE THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBxSz02zLPAd",
        "colab_type": "code",
        "outputId": "1d5660b7-f085-4d18-af05-9a78424dbf91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "# CREATE THE MODEL\n",
        "nGauss = 4\n",
        "model = Sequential()\n",
        "if nGauss>0:\n",
        "    #=============================================================================\n",
        "    model.add(GaussScaler(rank=2,filters=nGauss,kernel_size=(5,5), \n",
        "                         data_format='channels_last',strides=1,\n",
        "                         padding='same',name='gausslayer', activation='linear',\n",
        "                         input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))\n",
        "#=============================================================================\n",
        "else:\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu',input_shape=input_shape))\n",
        "  \n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        " \n",
        "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'gausslayer', 'input_shape': (28, 28, 1)}\n",
            "kernel shape: (5, 5, 1, 4)\n",
            "kernel shape in output: (5, 5, 4, 1)\n",
            "channel axis\n",
            "kernel reshaped : (5, 5, 4, 1)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gausslayer (GaussScaler)     (None, 28, 28, 4)         4         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 4)         16        \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        1184      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 144,606\n",
            "Trainable params: 144,470\n",
            "Non-trainable params: 136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyxCWeylLhff",
        "colab_type": "text"
      },
      "source": [
        "PLOT THE FILTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nZVKELWLgyT",
        "colab_type": "code",
        "outputId": "84cd4c4f-13ff-47db-bc36-6b2498c5cc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "plt = True and nGauss>0\n",
        "if plt:\n",
        "    print(\"Plotting kernels before...\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    gauss_layer = model.get_layer('gausslayer')\n",
        "    ws = gauss_layer.get_weights()\n",
        "    print(\"Sigmas before\",ws[0])\n",
        "    u_func = K.function(inputs=[model.input], outputs=[gauss_layer.U()])\n",
        "    output_func = K.function(inputs=[model.input], outputs=[gauss_layer.output])\n",
        "\n",
        "    U_val=u_func([np.expand_dims(x_test[0], axis=0)])\n",
        "    \n",
        "    print(\"U shape\", U_val[0].shape)\n",
        "    print(\"U max:\", np.max(U_val[0][:,:,:,:]))\n",
        "    num_filt=min(U_val[0].shape[2],12)\n",
        "    fig=plt.figure(figsize=(10,5))\n",
        "    for i in range(num_filt):\n",
        "        ax1=plt.subplot(1, num_filt, i+1)\n",
        "        im = ax1.imshow(np.squeeze(U_val[0][:,:,i,0]))\n",
        "    fig.colorbar(im, ax=ax1)\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plotting kernels before...\n",
            "Sigmas before [0.05       0.06666667 0.08333334 0.1       ]\n",
            "U shape (5, 5, 4, 1)\n",
            "U max: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAEnCAYAAAB8EZ0vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztpCKIbUS1JR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7033
        },
        "outputId": "f4185c1f-8465-44cb-d0e9-88ad5a05487e"
      },
      "source": [
        "from lr_multiplier import LearningRateMultiplier\n",
        "\n",
        "multipliers = {'gausslayer': 2.0}\n",
        "opt = LearningRateMultiplier(SGD, lr_multipliers=multipliers, \n",
        "                             lr=0.01, momentum=0.9,decay=0.000005000)\n",
        "# Higher decays hurt the process\n",
        "print(opt)\n",
        "#opt = SGD(lr=0.01,momentum=0.5)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<lr_multiplier.LearningRateMultiplier object at 0x7fe0926bf6d8>\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2802 - acc: 0.9159 - val_loss: 0.0656 - val_acc: 0.9797\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.1057 - acc: 0.9672 - val_loss: 0.0500 - val_acc: 0.9845\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0842 - acc: 0.9741 - val_loss: 0.0366 - val_acc: 0.9881\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0694 - acc: 0.9778 - val_loss: 0.0313 - val_acc: 0.9899\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0603 - acc: 0.9815 - val_loss: 0.0299 - val_acc: 0.9918\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0536 - acc: 0.9835 - val_loss: 0.0278 - val_acc: 0.9913\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0491 - acc: 0.9842 - val_loss: 0.0290 - val_acc: 0.9908\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0450 - acc: 0.9853 - val_loss: 0.0224 - val_acc: 0.9930\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0411 - acc: 0.9869 - val_loss: 0.0255 - val_acc: 0.9911\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0387 - acc: 0.9878 - val_loss: 0.0226 - val_acc: 0.9924\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0376 - acc: 0.9881 - val_loss: 0.0229 - val_acc: 0.9921\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0330 - acc: 0.9893 - val_loss: 0.0225 - val_acc: 0.9922\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0337 - acc: 0.9890 - val_loss: 0.0220 - val_acc: 0.9933\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0233 - val_acc: 0.9925\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0299 - acc: 0.9903 - val_loss: 0.0194 - val_acc: 0.9947\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0279 - acc: 0.9912 - val_loss: 0.0195 - val_acc: 0.9941\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0282 - acc: 0.9912 - val_loss: 0.0201 - val_acc: 0.9937\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0271 - acc: 0.9912 - val_loss: 0.0189 - val_acc: 0.9944\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0257 - acc: 0.9915 - val_loss: 0.0180 - val_acc: 0.9943\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0170 - val_acc: 0.9953\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0241 - acc: 0.9921 - val_loss: 0.0178 - val_acc: 0.9944\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0230 - acc: 0.9922 - val_loss: 0.0191 - val_acc: 0.9939\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0215 - acc: 0.9928 - val_loss: 0.0174 - val_acc: 0.9939\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0210 - acc: 0.9930 - val_loss: 0.0190 - val_acc: 0.9945\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.0180 - val_acc: 0.9942\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0187 - val_acc: 0.9945\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0197 - acc: 0.9939 - val_loss: 0.0177 - val_acc: 0.9949\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0170 - val_acc: 0.9951\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0159 - val_acc: 0.9949\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0180 - acc: 0.9938 - val_loss: 0.0164 - val_acc: 0.9943\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0164 - acc: 0.9945 - val_loss: 0.0167 - val_acc: 0.9954\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 0.0160 - val_acc: 0.9954\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0166 - val_acc: 0.9950\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0165 - acc: 0.9947 - val_loss: 0.0159 - val_acc: 0.9956\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0165 - val_acc: 0.9950\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0156 - acc: 0.9947 - val_loss: 0.0143 - val_acc: 0.9961\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0168 - val_acc: 0.9954\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0155 - acc: 0.9948 - val_loss: 0.0150 - val_acc: 0.9956\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0156 - val_acc: 0.9953\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0161 - val_acc: 0.9954\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0178 - val_acc: 0.9951\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0174 - val_acc: 0.9946\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0133 - acc: 0.9958 - val_loss: 0.0160 - val_acc: 0.9953\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0165 - val_acc: 0.9953\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.0167 - val_acc: 0.9955\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0140 - val_acc: 0.9960\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0174 - val_acc: 0.9952\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0154 - val_acc: 0.9954\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0128 - acc: 0.9955 - val_loss: 0.0155 - val_acc: 0.9955\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0172 - val_acc: 0.9952\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0156 - val_acc: 0.9950\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.0154 - val_acc: 0.9951\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0117 - acc: 0.9966 - val_loss: 0.0145 - val_acc: 0.9958\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0176 - val_acc: 0.9946\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0161 - val_acc: 0.9953\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0159 - val_acc: 0.9955\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0165 - val_acc: 0.9958\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0106 - acc: 0.9964 - val_loss: 0.0141 - val_acc: 0.9965\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.0151 - val_acc: 0.9959\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0176 - val_acc: 0.9954\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0103 - acc: 0.9964 - val_loss: 0.0156 - val_acc: 0.9959\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0144 - val_acc: 0.9966\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0155 - val_acc: 0.9963\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0141 - val_acc: 0.9957\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0088 - acc: 0.9972 - val_loss: 0.0147 - val_acc: 0.9962\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0141 - val_acc: 0.9961\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0091 - acc: 0.9968 - val_loss: 0.0147 - val_acc: 0.9953\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0153 - val_acc: 0.9952\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0135 - val_acc: 0.9959\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0159 - val_acc: 0.9955\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0146 - val_acc: 0.9961\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0157 - val_acc: 0.9960\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0157 - val_acc: 0.9958\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.0169 - val_acc: 0.9953\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0160 - val_acc: 0.9955\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0158 - val_acc: 0.9955\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0144 - val_acc: 0.9960\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0169 - val_acc: 0.9955\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0180 - val_acc: 0.9956\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0156 - val_acc: 0.9955\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0169 - val_acc: 0.9952\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0159 - val_acc: 0.9955\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0150 - val_acc: 0.9958\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0149 - val_acc: 0.9956\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0181 - val_acc: 0.9952\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.0145 - val_acc: 0.9957\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.0151 - val_acc: 0.9962\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0149 - val_acc: 0.9960\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0155 - val_acc: 0.9961\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0148 - val_acc: 0.9954\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0142 - val_acc: 0.9961\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0194 - val_acc: 0.9946\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0157 - val_acc: 0.9960\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0172 - val_acc: 0.9957\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0153 - val_acc: 0.9957\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0147 - val_acc: 0.9958\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0063 - acc: 0.9975 - val_loss: 0.0152 - val_acc: 0.9956\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0145 - val_acc: 0.9962\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0162 - val_acc: 0.9958\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0163 - val_acc: 0.9955\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9959\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0169 - val_acc: 0.9952\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0165 - val_acc: 0.9956\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9954\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0169 - val_acc: 0.9954\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0160 - val_acc: 0.9953\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0170 - val_acc: 0.9956\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0153 - val_acc: 0.9961\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0164 - val_acc: 0.9956\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0164 - val_acc: 0.9955\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0153 - val_acc: 0.9957\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.0184 - val_acc: 0.9948\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0153 - val_acc: 0.9953\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0149 - val_acc: 0.9955\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0157 - val_acc: 0.9957\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0161 - val_acc: 0.9957\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9959\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0148 - val_acc: 0.9957\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0153 - val_acc: 0.9958\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0159 - val_acc: 0.9959\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0161 - val_acc: 0.9957\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0160 - val_acc: 0.9962\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0145 - val_acc: 0.9960\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0153 - val_acc: 0.9958\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0151 - val_acc: 0.9963\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0167 - val_acc: 0.9957\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0157 - val_acc: 0.9961\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0155 - val_acc: 0.9963\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0160 - val_acc: 0.9958\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0169 - val_acc: 0.9952\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0145 - val_acc: 0.9960\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0153 - val_acc: 0.9960\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0159 - val_acc: 0.9956\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0163 - val_acc: 0.9955\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0170 - val_acc: 0.9958\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0162 - val_acc: 0.9959\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0049 - acc: 0.9982 - val_loss: 0.0165 - val_acc: 0.9957\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0152 - val_acc: 0.9958\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0173 - val_acc: 0.9955\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0168 - val_acc: 0.9955\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0175 - val_acc: 0.9958\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0173 - val_acc: 0.9958\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0154 - val_acc: 0.9957\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0160 - val_acc: 0.9957\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0158 - val_acc: 0.9955\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0166 - val_acc: 0.9955\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0163 - val_acc: 0.9960\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0158 - val_acc: 0.9957\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0160 - val_acc: 0.9961\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0156 - val_acc: 0.9958\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0166 - val_acc: 0.9956\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0161 - val_acc: 0.9960\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0171 - val_acc: 0.9954\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0160 - val_acc: 0.9959\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0174 - val_acc: 0.9957\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0174 - val_acc: 0.9957\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0152 - val_acc: 0.9954\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0153 - val_acc: 0.9961\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0144 - val_acc: 0.9960\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0150 - val_acc: 0.9958\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0157 - val_acc: 0.9956\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0142 - val_acc: 0.9961\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0149 - val_acc: 0.9961\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0153 - val_acc: 0.9960\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0158 - val_acc: 0.9964\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0151 - val_acc: 0.9964\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0156 - val_acc: 0.9964\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0170 - val_acc: 0.9959\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0172 - val_acc: 0.9953\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0165 - val_acc: 0.9953\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0159 - val_acc: 0.9954\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0175 - val_acc: 0.9953\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0159 - val_acc: 0.9962\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0156 - val_acc: 0.9959\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0165 - val_acc: 0.9961\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0153 - val_acc: 0.9959\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0164 - val_acc: 0.9953\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0158 - val_acc: 0.9956\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0169 - val_acc: 0.9958\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0173 - val_acc: 0.9956\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0156 - val_acc: 0.9960\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0152 - val_acc: 0.9958\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0165 - val_acc: 0.9956\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0173 - val_acc: 0.9959\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0162 - val_acc: 0.9955\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0156 - val_acc: 0.9957\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0170 - val_acc: 0.9959\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0155 - val_acc: 0.9957\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0169 - val_acc: 0.9955\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0168 - val_acc: 0.9956\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0165 - val_acc: 0.9958\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0170 - val_acc: 0.9953\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0176 - val_acc: 0.9953\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0163 - val_acc: 0.9957\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0179 - val_acc: 0.9950\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0175 - val_acc: 0.9955\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0166 - val_acc: 0.9955\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0178 - val_acc: 0.9958\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0169 - val_acc: 0.9958\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0179 - val_acc: 0.9962\n",
            "Test loss: 0.017914599338304492\n",
            "Test accuracy: 0.9962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF4aI5BOLsqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "978b4f1a-ba41-48b3-f3af-09b4dd8e9753"
      },
      "source": [
        "if plt:\n",
        "    print(\"Plotting kernels after ...\")\n",
        "    \n",
        "    print(\"U max:\", np.max(U_val[0][:,:,:,:]))\n",
        "    import matplotlib.pyplot as plt\n",
        "    ws = gauss_layer.get_weights()\n",
        "    print(\"Sigmas after\",ws[0])\n",
        "    U_val=u_func([np.expand_dims(x_test[2], axis=0)])\n",
        "    \n",
        "    print(\"U shape\", U_val[0].shape)\n",
        "    num_filt=min(U_val[0].shape[2],12)\n",
        "    fig=plt.figure(figsize=(16,5))\n",
        "    for i in range(num_filt):\n",
        "        ax=plt.subplot(1, num_filt, i+1)\n",
        "        im = ax.imshow(np.squeeze(U_val[0][:,:,i,0]))\n",
        "    #fig.colorbar(im, ax=ax1)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    print(\"outputs  ...\")\n",
        "    \n",
        "    n=5\n",
        "    \n",
        "    out_val=output_func([np.expand_dims(x_test[5], axis=0)])\n",
        "    print(\"Outputs shape\", out_val[0].shape)\n",
        "    num_filt=min(out_val[0].shape[3],12)\n",
        "    fig=plt.figure(figsize=(16,10))\n",
        "    ax=plt.subplot(1, num_filt+1, 1)\n",
        "    im = ax.imshow(np.squeeze(x_test[5]))\n",
        "    print(\"input mean,var,max\",np.mean(x_test[5]),np.var(x_test[5]),np.max(x_test[5]))\n",
        "    for i in range(num_filt):\n",
        "        ax=plt.subplot(1, num_filt+1, i+2)\n",
        "        out_im = out_val[0][0,:,:,i]\n",
        "        im = ax.imshow(np.squeeze(out_im))\n",
        "        print(\"ouput mean,var,max\",np.mean(out_im),\n",
        "                                       np.var(out_im),\n",
        "                                       np.max(out_im))\n",
        "        #plt.colorbar(im,ax=ax)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plotting kernels after ...\n",
            "U max: 1.0\n",
            "Sigmas after [ 0.04040195 -0.          0.9171165   0.3766555 ]\n",
            "U shape (5, 5, 4, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5wAAADkCAYAAAD0KyvgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABaGJwAAAC0+F9/6WIipVbmlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x360 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "outputs  ...\n",
            "Outputs shape (1, 28, 28, 4)\n",
            "input mean,var,max 0.06930272 0.054472942 0.99607843\n",
            "ouput mean,var,max 0.06930272 0.054472942 0.99607843\n",
            "ouput mean,var,max 0.06930272 0.054472942 0.99607843\n",
            "ouput mean,var,max 0.34520313 0.6071919 3.1693373\n",
            "ouput mean,var,max 0.3105209 0.5663452 3.300778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAC8CAYAAABizBPxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1152x720 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTuRWucxLT-q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}